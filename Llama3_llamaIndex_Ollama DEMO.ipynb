{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary packages\n",
    "\n",
    "Run the following commands to install the necessary packages:\n",
    "\n",
    "Before downloading: <u><b>Download Llama3 with Ollama</b></u>\n",
    "\n",
    "```bash\n",
    "!pip install llama-index==0.10.32  # Version: 0.10.32\n",
    "!pip install llama-index-core==0.10.32  # Version: 0.10.32\n",
    "!pip install llama-index-llms-ollama==0.1.2  # Version: 0.1.2\n",
    "!pip install llama-index-readers-web==0.1.10  # Version: 0.1.10\n",
    "!pip install llama-index-embeddings-ollama==0.1.2  # Version: 0.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama Index Documentation\n",
    "\n",
    "Here are some useful links to get started with Llama Index:\n",
    "\n",
    "1. [Starter Example Local](https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/)\n",
    "2. [Data Connectors - Web Page Demo](https://docs.llamaindex.ai/en/stable/examples/data_connectors/WebPageDemo/)\n",
    "3. [Embeddings - Ollama Embedding](https://docs.llamaindex.ai/en/stable/examples/embeddings/ollama_embedding/)\n",
    "4. [Model - Llama3 Download with Ollama ](https://ollama.com/library/llama3)\n",
    "\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Load data and build an index - <span style=\"color:LightSkyBlue\">WEB</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://ai.meta.com/blog/meta-llama-3/\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='https://ai.meta.com/blog/meta-llama-3/', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='![Meta](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/252294889_575082167077436_6034106545912333281_n.svg/meta-\\nlogo-\\nprimary_standardsize.svg?_nc_cat=1&ccb=1-7&_nc_sid=e280be&_nc_ohc=u-d6BJPuy1sQ7kNvgEwLGFW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfBCAo8xwiukB5iTxBoB-7dn8Lj89xmbO-Kr_FewrDHLOA&oe=663BE539)\\n\\n* Our approach\\n* Research\\n* [Meta AI](/meta-ai/)\\n* [Meta Llama](https://llama.meta.com/)\\n* [Blog](/blog/)\\n* [Try Meta AI](https://www.meta.ai/?utm_source=ai_meta_site&utm_medium=web&utm_content=AI_nav&utm_campaign=April_moment)\\n* [](/)\\n\\nLarge Language Model\\n\\nIntroducing Meta Llama 3: The most capable openly available LLM to date\\n\\nApril 18, 2024\\n\\n  \\n\\nTakeaways:\\n\\nRECOMMENDED READS\\n\\n  * [5 Steps to Getting Started with Llama 2](https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/)\\n  * [The Llama Ecosystem: Past, Present, and Future](https://ai.meta.com/blog/llama-2-updates-connect-2023/)\\n  * [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/)\\n  * [Meta and Microsoft Introduce the Next Generation of Llama](https://ai.meta.com/blog/llama-2/)\\n\\n  \\n\\n  * Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.\\n  * Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.\\n  * We’re dedicated to developing Llama 3 in a responsible way, and we’re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.\\n  * In the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we’ll share the Llama 3 research paper.\\n  * Meta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI [_here_](https://meta.ai/).\\n\\n  \\n\\nToday, we’re excited to share the first two models of the next generation of\\nLlama, Meta Llama 3, available for broad use. This release features pretrained\\nand instruction-fine-tuned language models with 8B and 70B parameters that can\\nsupport a broad range of use cases. This next generation of Llama demonstrates\\nstate-of-the-art performance on a wide range of industry benchmarks and offers\\nnew capabilities, including improved reasoning. We believe these are the best\\nopen source models of their class, period. In support of our longstanding open\\napproach, we’re putting Llama 3 in the hands of the community. We want to\\nkickstart the next wave of innovation in AI across the stack—from applications\\nto developer tools to evals to inference optimizations and more. We can’t wait\\nto see what you build and look forward to your feedback.\\n\\nOur goals for Llama 3\\n\\n  \\n\\nWith Llama 3, we set out to build the best open models that are on par with\\nthe best proprietary models available today. We wanted to address developer\\nfeedback to increase the overall helpfulness of Llama 3 and are doing so while\\ncontinuing to play a leading role on responsible use and deployment of LLMs.\\nWe are embracing the open source ethos of releasing early and often to enable\\nthe community to get access to these models while they are still in\\ndevelopment. The text-based models we are releasing today are the first in the\\nLlama 3 collection of models. Our goal in the near future is to make Llama 3\\nmultilingual and multimodal, have longer context, and continue to improve\\noverall performance across core LLM capabilities such as reasoning and coding.\\n\\nState-of-the-art performance\\n\\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and\\nestablish a new state-of-the-art for LLM models at those scales. Thanks to\\nimprovements in pretraining and post-training, our pretrained and instruction-\\nfine-tuned models are the best models existing today at the 8B and 70B\\nparameter scale. Improvements in our post-training procedures substantially\\nreduced false refusal rates, improved alignment, and increased diversity in\\nmodel responses. We also saw greatly improved capabilities like reasoning,\\ncode generation, and instruction following making Llama 3 more steerable.\\n\\n  \\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/438037375_405784438908376_6082258861354187544_n.png?_nc_cat=106&ccb=1-7&_nc_sid=e280be&_nc_ohc=t8bz_2VzTMIQ7kNvgH-V8dn&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCF34l-Cvh8UBaCbu2dhFjByK_-u5fGmnxAVbj1SvIBtQ&oe=6650498A)\\n\\n*Please see [_evaluation details_](https://github.com/meta-llama/llama3/blob/main/eval_details.md) for setting and parameters with which these evaluations are calculated.\\n\\n  \\n\\nIn the development of Llama 3, we looked at model performance on standard\\nbenchmarks and also sought to optimize for performance for real-world\\nscenarios. To this end, we developed a new high-quality human evaluation set.\\nThis evaluation set contains 1,800 prompts that cover 12 key use cases: asking\\nfor advice, brainstorming, classification, closed question answering, coding,\\ncreative writing, extraction, inhabiting a character/persona, open question\\nanswering, reasoning, rewriting, and summarization. To prevent accidental\\noverfitting of our models on this evaluation set, even our own modeling teams\\ndo not have access to it. The chart below shows aggregated results of our\\nhuman evaluations across of these categories and prompts against Claude\\nSonnet, Mistral Medium, and GPT-3.5.\\n\\n  \\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/438998263_1368970367138244_7396600838045603809_n.png?_nc_cat=111&ccb=1-7&_nc_sid=e280be&_nc_ohc=O8e1uTklrugQ7kNvgETcJfK&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB3eYFDPpjhm34Die132DRE8hfjOSQJBAOu4PfQpifiSw&oe=66502ECF)\\n\\nPreference rankings by human annotators based on this evaluation set highlight\\nthe strong performance of our 70B instruction-following model compared to\\ncompeting models of comparable size in real-world scenarios.\\n\\nOur pretrained model also establishes a new state-of-the-art for LLM models at\\nthose scales.\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/439014085_432870519293677_8138616034495713484_n.png?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=HQcHYNfUefgQ7kNvgGRCn56&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfC0SUXlt95yR__JVbqHeOT3_UnJRsbtbwh1XnLY2sQbxw&oe=665050B4)\\n\\n*Please see [_evaluation details_](https://github.com/meta-llama/llama3/blob/main/eval_details.md) for setting and parameters with which these evaluations are calculated.\\n\\n  \\n\\nTo develop a great language model, we believe it’s important to innovate,\\nscale, and optimize for simplicity. We adopted this design philosophy\\nthroughout the Llama 3 project with a focus on four key ingredients: the model\\narchitecture, the pretraining data, scaling up pretraining, and instruction\\nfine-tuning.\\n\\nModel architecture\\n\\nIn line with our design philosophy, we opted for a relatively standard\\ndecoder-only transformer architecture in Llama 3. Compared to Llama 2, we made\\nseveral key improvements. Llama 3 uses a tokenizer with a vocabulary of 128K\\ntokens that encodes language much more efficiently, which leads to\\nsubstantially improved model performance. To improve the inference efficiency\\nof Llama 3 models, we’ve adopted grouped query attention (GQA) across both the\\n8B and 70B sizes. We trained the models on sequences of 8,192 tokens, using a\\nmask to ensure self-attention does not cross document boundaries.\\n\\nTraining data\\n\\nTo train the best language model, the curation of a large, high-quality\\ntraining dataset is paramount. In line with our design principles, we invested\\nheavily in pretraining data. Llama 3 is pretrained on over 15T tokens that\\nwere all collected from publicly available sources. Our training dataset is\\nseven times larger than that used for Llama 2, and it includes four times more\\ncode. To prepare for upcoming multilingual use cases, over 5% of the Llama 3\\npretraining dataset consists of high-quality non-English data that covers over\\n30 languages. However, we do not expect the same level of performance in these\\nlanguages as in English.\\n\\nTo ensure Llama 3 is trained on data of the highest quality, we developed a\\nseries of data-filtering pipelines. These pipelines include using heuristic\\nfilters, NSFW filters, semantic deduplication approaches, and text classifiers\\nto predict data quality. We found that previous generations of Llama are\\nsurprisingly good at identifying high-quality data, hence we used Llama 2 to\\ngenerate the training data for the text-quality classifiers that are powering\\nLlama 3.\\n\\nWe also performed extensive experiments to evaluate the best ways of mixing\\ndata from different sources in our final pretraining dataset. These\\nexperiments enabled us to select a data mix that ensures that Llama 3 performs\\nwell across use cases including trivia questions, STEM, coding, historical\\nknowledge, _etc._\\n\\nScaling up pretraining\\n\\nTo effectively leverage our pretraining data in Llama 3 models, we put\\nsubstantial effort into scaling up pretraining. Specifically, we have\\ndeveloped a series of detailed scaling laws for downstream benchmark\\nevaluations. These scaling laws enable us to select an optimal data mix and to\\nmake informed decisions on how to best use our training compute. Importantly,\\nscaling laws allow us to predict the performance of our largest models on key\\ntasks (for example, code generation as evaluated on the HumanEval\\nbenchmark—see above) before we actually train the models. This helps us ensure\\nstrong performance of our final models across a variety of use cases and\\ncapabilities.\\n\\nWe made several new observations on scaling behavior during the development of\\nLlama 3. For example, while the Chinchilla-optimal amount of training compute\\nfor an 8B parameter model corresponds to ~200B tokens, we found that model\\nperformance continues to improve even after the model is trained on two orders\\nof magnitude more data. Both our 8B and 70B parameter models continued to\\nimprove log-linearly after we trained them on up to 15T tokens. Larger models\\ncan match the performance of these smaller models with less training compute,\\nbut smaller models are generally preferred because they are much more\\nefficient during inference.\\n\\nTo train our largest Llama 3 models, we combined three types of\\nparallelization: data parallelization, model parallelization, and pipeline\\nparallelization. Our most efficient implementation achieves a compute\\nutilization of over 400 TFLOPS per GPU when trained on 16K GPUs\\nsimultaneously. We performed training runs on two custom-built [_24K GPU\\nclusters_](https://engineering.fb.com/2024/03/12/data-center-\\nengineering/building-metas-genai-infrastructure/). To maximize GPU uptime, we\\ndeveloped an advanced new training stack that automates error detection,\\nhandling, and maintenance. We also greatly improved our hardware reliability\\nand detection mechanisms for silent data corruption, and we developed new\\nscalable storage systems that reduce overheads of checkpointing and rollback.\\nThose improvements resulted in an overall effective training time of more than\\n95%. Combined, these improvements increased the efficiency of Llama 3 training\\nby ~three times compared to Llama 2.\\n\\nInstruction fine-tuning\\n\\nTo fully unlock the potential of our pretrained models in chat use cases, we\\ninnovated on our approach to instruction-tuning as well. Our approach to post-\\ntraining is a combination of supervised fine-tuning (SFT), rejection sampling,\\nproximal policy optimization (PPO), and direct preference optimization (DPO).\\nThe quality of the prompts that are used in SFT and the preference rankings\\nthat are used in PPO and DPO has an outsized influence on the performance of\\naligned models. Some of our biggest improvements in model quality came from\\ncarefully curating this data and performing multiple rounds of quality\\nassurance on annotations provided by human annotators.\\n\\nLearning from preference rankings via PPO and DPO also greatly improved the\\nperformance of Llama 3 on reasoning and coding tasks. We found that if you ask\\na model a reasoning question that it struggles to answer, the model will\\nsometimes produce the right reasoning trace: The model knows how to produce\\nthe right answer, but it does not know how to select it. Training on\\npreference rankings enables the model to learn how to select it.\\n\\nBuilding with Llama 3\\n\\nOur vision is to enable developers to customize Llama 3 to support relevant\\nuse cases and to make it easier to adopt best practices and improve the open\\necosystem. With this release, we’re providing new trust and safety tools\\nincluding updated [_components_](https://github.com/meta-llama/PurpleLlama)\\nwith both Llama Guard 2 and Cybersec Eval 2, and the introduction of Code\\nShield—an inference time guardrail for filtering insecure code produced by\\nLLMs.\\n\\nWe’ve also co-developed Llama 3 with\\n[_torchtune_](https://github.com/pytorch/torchtune), the new PyTorch-native\\nlibrary for easily authoring, fine-tuning, and experimenting with LLMs.\\ntorchtune provides memory efficient and hackable training recipes written\\nentirely in PyTorch. The library is integrated with popular platforms such as\\nHugging Face, Weights & Biases, and EleutherAI and even supports Executorch\\nfor enabling efficient inference to be run on a wide variety of mobile and\\nedge devices. For everything from prompt engineering to using Llama 3 with\\nLangChain we have a comprehensive [_getting started\\nguide_](https://llama.meta.com/get-started/) and takes you from downloading\\nLlama 3 all the way to deployment at scale within your generative AI\\napplication.\\n\\nA system-level approach to responsibility\\n\\nWe have designed Llama 3 models to be maximally helpful while ensuring an\\nindustry leading approach to responsibly deploying them. To achieve this, we\\nhave adopted [_a new, system-level approach_](https://ai.meta.com/blog/meta-\\nllama-3-meta-ai-responsibility/) to the responsible development and deployment\\nof Llama. We envision Llama models as part of a broader system that puts the\\ndeveloper in the driver’s seat. Llama models will serve as a foundational\\npiece of a system that developers design with their unique end goals in mind.\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/438922663_1135166371264105_805978695964769385_n.png?_nc_cat=107&ccb=1-7&_nc_sid=e280be&_nc_ohc=TmMIuayfFZUQ7kNvgGNdHtq&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfD7qchG3Qj5OHB3mSkuFi6FAjIv2DCFYV6BiYmQcMvLCw&oe=6650375E)\\n\\nInstruction fine-tuning also plays a major role in ensuring the safety of our\\nmodels. Our instruction-fine-tuned models have been red-teamed (tested) for\\nsafety through internal and external efforts. \\u200b\\u200bOur red teaming approach\\nleverages human experts and automation methods to generate adversarial prompts\\nthat try to elicit problematic responses. For instance, we apply comprehensive\\ntesting to assess risks of misuse related to Chemical, Biological, Cyber\\nSecurity, and other risk areas. All of these efforts are iterative and used to\\ninform safety fine-tuning of the models being released. You can read more\\nabout our efforts in the [_model card_](https://github.com/meta-\\nllama/llama3/blob/main/MODEL_CARD.md).\\n\\nLlama Guard models are meant to be a foundation for prompt and response safety\\nand can easily be fine-tuned to create a new taxonomy depending on application\\nneeds. As a starting point, the new Llama Guard 2 uses the recently\\n[_announced_](https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/) MLCommons\\ntaxonomy, in an effort to support the emergence of industry standards in this\\nimportant area. Additionally, CyberSecEval 2 expands on its predecessor by\\nadding measures of an LLM’s propensity to allow for abuse of its code\\ninterpreter, offensive cybersecurity capabilities, and susceptibility to\\nprompt injection attacks (learn more in [_our technical\\npaper_](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-\\nranging-cybersecurity-evaluation-suite-for-large-language-models/)). Finally,\\nwe’re introducing Code Shield which adds support for inference-time filtering\\nof insecure code produced by LLMs. This offers mitigation of risks around\\ninsecure code suggestions, code interpreter abuse prevention, and secure\\ncommand execution.\\n\\nWith the speed at which the generative AI space is moving, we believe an open\\napproach is an important way to bring the ecosystem together and mitigate\\nthese potential harms. As part of that, we’re updating our [_Responsible Use\\nGuide_](https://llama.meta.com/responsible-use-guide) (RUG) that provides a\\ncomprehensive guide to responsible development with LLMs. As we outlined in\\nthe RUG, we recommend that all inputs and outputs be checked and filtered in\\naccordance with content guidelines appropriate to the application.\\nAdditionally, many cloud service providers offer content moderation APIs and\\nother tools for responsible deployment, and we encourage developers to also\\nconsider using these options.\\n\\nDeploying Llama 3 at scale\\n\\nLlama 3 will soon be available on all major platforms including cloud\\nproviders, model API providers, and much more. Llama 3 will be\\n[_everywhere_](https://llama.meta.com/get-started/).\\n\\nOur benchmarks show the tokenizer offers improved token efficiency, yielding\\nup to 15% fewer tokens compared to Llama 2. Also, Group Query Attention (GQA)\\nnow has been added to Llama 3 8B as well. As a result, we observed that\\ndespite the model having 1B more parameters compared to Llama 2 7B, the\\nimproved tokenizer efficiency and GQA contribute to maintaining the inference\\nefficiency on par with Llama 2 7B.\\n\\nFor examples of how to leverage all of these capabilities, check out [_Llama\\nRecipes_](https://github.com/meta-llama/llama-recipes) which contains all of\\nour open source code that can be leveraged for everything from fine-tuning to\\ndeployment to model evaluation.\\n\\nWhat’s next for Llama 3?\\n\\nThe Llama 3 8B and 70B models mark the beginning of what we plan to release\\nfor Llama 3. And there’s a lot more to come.\\n\\nOur largest models are over 400B parameters and, while these models are still\\ntraining, our team is excited about how they’re trending. Over the coming\\nmonths, we’ll release multiple models with new capabilities including\\nmultimodality, the ability to converse in multiple languages, a much longer\\ncontext window, and stronger overall capabilities. We will also publish a\\ndetailed research paper once we are done training Llama 3.\\n\\nTo give you a sneak preview for where these models are today as they continue\\ntraining, we thought we could share some snapshots of how our largest LLM\\nmodel is trending. Please note that this data is based on an early checkpoint\\nof Llama 3 that is still training and these capabilities are not supported as\\npart of the models released today.\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/439015366_1603174683862748_5008894608826037916_n.png?_nc_cat=105&ccb=1-7&_nc_sid=e280be&_nc_ohc=pzX9JxbuTFMQ7kNvgGtuFp6&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfBjH7xTgW_i1BxFuAAEvK1pOfIaZO6FK6--wR_iCBJBFA&oe=6650526A)\\n\\n*Please see [_evaluation details_](https://github.com/meta-llama/llama3/blob/main/eval_details.md) for setting and parameters with which these evaluations are calculated.\\n\\n  \\n\\nWe’re committed to the continued growth and development of an open AI\\necosystem for releasing our models responsibly. We have long believed that\\nopenness leads to better, safer products, faster innovation, and a healthier\\noverall market. This is good for Meta, and it is good for society. We’re\\ntaking a community-first approach with Llama 3, and starting today, these\\nmodels are available on the leading cloud, hosting, and hardware platforms\\nwith many more to come.\\n\\nTry Meta Llama 3 today\\n\\nWe’ve integrated our latest models into Meta AI, which we believe is the\\nworld’s leading AI assistant. It’s now built with Llama 3 technology and it’s\\navailable in more countries across our apps.\\n\\nYou can use Meta AI on Facebook, Instagram, WhatsApp, Messenger, and [_the\\nweb_](https://meta.ai/) to get things done, learn, create, and connect with\\nthe things that matter to you. You can read more about the Meta AI experience\\n[_here_](https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-\\nllama-3/).\\n\\nVisit the[ _Llama 3 website_](https://llama.meta.com/llama3) to download the\\nmodels and reference the[ _Getting Started Guide_](https://llama.meta.com/get-\\nstarted/) for the latest list of all available platforms.\\n\\nYou’ll also soon be able to test multimodal Meta AI on our Ray-Ban Meta smart\\nglasses.\\n\\nAs always, we look forward to seeing all the amazing products and experiences\\nyou will build with Meta Llama 3.\\n\\n* * *\\n\\nShare:[](https://www.facebook.com/sharer/sharer.php?u=https://ai.meta.com/blog/meta-\\nllama-3/)[](https://www.twitter.com/share?url=https://ai.meta.com/blog/meta-\\nllama-3/)[](https://www.linkedin.com/sharing/share-\\noffsite?url=https://ai.meta.com/blog/meta-llama-3/)\\n\\n* * *\\n\\nOur latest updates delivered to your inbox\\n\\n[Subscribe](https://ai.facebook.com/subscribe/) to our newsletter to keep up\\nwith Meta AI news, events, research breakthroughs, and more.\\n\\nJoin us in the pursuit of what’s possible with AI.\\n\\n[See all open\\npositions](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams%5B0%5D=Artificial+Intelligence&is_in_page=0&fbclid=IwAR0O8BF7opOj5gASJmwYVGalPPXTLu-6xrl9w00eC7Rarp2HQ9uEH8tERFw)\\n\\nRelated Posts\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/338318848_238475658638014_6444534044370711549_n.gif?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=i1sT3MB0-r4Q7kNvgHSkM8N&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfADsVCPxcDcyvdrbOxchiv6tMM4u8f_9Ltxk_7jhM0bEQ&oe=66505CA9)\\n\\nComputer Vision\\n\\nIntroducing Segment Anything: Working toward the first foundation model for\\nimage segmentation\\n\\nApril 5, 2023\\n\\n[Read post](https://ai.meta.com/blog/segment-anything-foundation-model-image-\\nsegmentation/)\\n\\nFEATURED\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/284099254_760295688673506_1047420741523524710_n.jpg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=5AkQ8HUNwRMQ7kNvgHmOeNm&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDtSEZLpr7SQlTI5LTB_HOV_Gct2PgJMBRbAZ9oqD5Jjw&oe=665030F6)\\n\\nResearch\\n\\nMultiRay: Optimizing efficiency for large-scale AI models\\n\\nNovember 18, 2022\\n\\n[Read post](https://ai.meta.com/blog/multiray-large-scale-AI-models/)\\n\\nFEATURED\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/334793505_583125787173687_542838236294006040_n.png?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=EnfdMS9TfpYQ7kNvgFkLm2O&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfBZFwGiagYXpJJOHsaKiiPyMq4OLVEL_3repwvAVHZ9Jg&oe=66504484)\\n\\nML Applications\\n\\nMuAViC: The first audio-video speech translation benchmark\\n\\nMarch 8, 2023\\n\\n[Read post](https://ai.meta.com/blog/muavic-audio-visual-speech-translation-\\nbenchmark/)\\n\\n[Our approach](/about) __ __\\n\\n[About AI at Meta](/about)\\n\\n[ Responsibility](/responsible-ai)\\n\\n[People](/results/?content_types%5B0%5D=person&sort_by=random)\\n\\n[Careers](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams\\\\[0\\\\]=Artificial%20Intelligence&is_in_page=0)\\n\\n[Research](/research) __ __\\n\\n[Infrastructure](/infrastructure)\\n\\n[ Resources](/resources)\\n\\n[Demos](/resources/demos/)\\n\\nProduct experiences\\n\\n __ __\\n\\n[Meta AI](/meta-ai/)\\n\\n[ Latest news](/blog) __ __\\n\\n[Blog](/blog)\\n\\n[ Newsletter](/subscribe)\\n\\nFoundational models\\n\\n __ __\\n\\n[Meta Llama](https://llama.meta.com/)\\n\\n![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.2365-6/87524316_2677189655726266_6338721200264445952_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=MD3GjYrvdigQ7kNvgFUHSa1&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfD0J8B6W72mTCh_6pVz9BmKBPqsgp5xRqr5DfOLWxddOg&oe=66504EF8)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tH5WJzdd1AgQ7kNvgHJHDmi&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB87HqTSVOu-7ypgKms-\\nXFN7BoeglYJCN80U3p76TV3gg&oe=663BD567)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tH5WJzdd1AgQ7kNvgHJHDmi&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB87HqTSVOu-7ypgKms-\\nXFN7BoeglYJCN80U3p76TV3gg&oe=663BD567)](https://www.facebook.com/aiatmeta/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=jJ34yUSmfSYQ7kNvgH5uro7&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfAZ1iIqIFnXn66BrOuXdE5lHMRorY9CKFy7nT9AWxlixg&oe=663BCDA2)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=jJ34yUSmfSYQ7kNvgH5uro7&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfAZ1iIqIFnXn66BrOuXdE5lHMRorY9CKFy7nT9AWxlixg&oe=663BCDA2)](https://twitter.com/aiatmeta/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgG12PhW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCWhxjaAmPTaO10a4Ngb6UR4Cuw8W3n6J9mfSEMv4YuxA&oe=663BC13B)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgG12PhW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCWhxjaAmPTaO10a4Ngb6UR4Cuw8W3n6J9mfSEMv4YuxA&oe=663BC13B)](https://www.linkedin.com/showcase/aiatmeta)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHHBFlF&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDqM1xJI106TeaRdK15NfgIpxKTyfVknIZ6HzCgSrnAxg&oe=663BDAAE)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHHBFlF&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDqM1xJI106TeaRdK15NfgIpxKTyfVknIZ6HzCgSrnAxg&oe=663BDAAE)](https://www.youtube.com/@aiatmeta)\\n\\nOur approach\\n\\n __ __\\n\\n[Our approach](/about)[ About AI at\\nMeta](/about)[Responsibility](/responsible-\\nai)[People](/results/?content_types%5B0%5D=person&sort_by=random)[Careers](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams\\\\[0\\\\]=Artificial%20Intelligence&is_in_page=0)\\n\\nResearch\\n\\n __ __\\n\\n[Research](/research)[\\nInfrastructure](/infrastructure)[Resources](/resources)[Demos](/resources/demos/)\\n\\nProduct experiences\\n\\n __ __\\n\\n[Meta AI](/meta-ai/)\\n\\nLatest news\\n\\n __ __\\n\\n[Latest news](/blog)[ Blog](/blog)[Newsletter](/subscribe)\\n\\nFoundational models\\n\\n __ __\\n\\n[Meta Llama](https://llama.meta.com/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tH5WJzdd1AgQ7kNvgHJHDmi&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB87HqTSVOu-7ypgKms-\\nXFN7BoeglYJCN80U3p76TV3gg&oe=663BD567)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tH5WJzdd1AgQ7kNvgHJHDmi&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB87HqTSVOu-7ypgKms-\\nXFN7BoeglYJCN80U3p76TV3gg&oe=663BD567)](https://www.facebook.com/aiatmeta/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=jJ34yUSmfSYQ7kNvgH5uro7&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfAZ1iIqIFnXn66BrOuXdE5lHMRorY9CKFy7nT9AWxlixg&oe=663BCDA2)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=jJ34yUSmfSYQ7kNvgH5uro7&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfAZ1iIqIFnXn66BrOuXdE5lHMRorY9CKFy7nT9AWxlixg&oe=663BCDA2)](https://twitter.com/aiatmeta/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgG12PhW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCWhxjaAmPTaO10a4Ngb6UR4Cuw8W3n6J9mfSEMv4YuxA&oe=663BC13B)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgG12PhW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCWhxjaAmPTaO10a4Ngb6UR4Cuw8W3n6J9mfSEMv4YuxA&oe=663BC13B)](https://www.linkedin.com/showcase/aiatmeta)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHHBFlF&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDqM1xJI106TeaRdK15NfgIpxKTyfVknIZ6HzCgSrnAxg&oe=663BDAAE)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHHBFlF&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDqM1xJI106TeaRdK15NfgIpxKTyfVknIZ6HzCgSrnAxg&oe=663BDAAE)](https://www.youtube.com/@aiatmeta)\\n\\n[ Privacy Policy](https://www.facebook.com/about/privacy/)\\n\\n[Terms](https://www.facebook.com/policies/)\\n\\n[Cookies](https://www.facebook.com/policies/cookies/)\\n\\nMeta © 2024\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tH5WJzdd1AgQ7kNvgHJHDmi&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB87HqTSVOu-7ypgKms-\\nXFN7BoeglYJCN80U3p76TV3gg&oe=663BD567)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=tH5WJzdd1AgQ7kNvgHJHDmi&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfB87HqTSVOu-7ypgKms-\\nXFN7BoeglYJCN80U3p76TV3gg&oe=663BD567)](https://www.facebook.com/aiatmeta/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=jJ34yUSmfSYQ7kNvgH5uro7&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfAZ1iIqIFnXn66BrOuXdE5lHMRorY9CKFy7nT9AWxlixg&oe=663BCDA2)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=jJ34yUSmfSYQ7kNvgH5uro7&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfAZ1iIqIFnXn66BrOuXdE5lHMRorY9CKFy7nT9AWxlixg&oe=663BCDA2)](https://twitter.com/aiatmeta/)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgG12PhW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCWhxjaAmPTaO10a4Ngb6UR4Cuw8W3n6J9mfSEMv4YuxA&oe=663BC13B)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgG12PhW&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfCWhxjaAmPTaO10a4Ngb6UR4Cuw8W3n6J9mfSEMv4YuxA&oe=663BC13B)](https://www.linkedin.com/showcase/aiatmeta)\\n\\n[![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHHBFlF&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDqM1xJI106TeaRdK15NfgIpxKTyfVknIZ6HzCgSrnAxg&oe=663BDAAE)![](https://scontent-\\narn2-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHHBFlF&_nc_ht=scontent-\\narn2-1.xx&oh=00_AfDqM1xJI106TeaRdK15NfgIpxKTyfVknIZ6HzCgSrnAxg&oe=663BDAAE)](https://www.youtube.com/@aiatmeta)\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create <u><b>Llama3</b></u> model with <span style=\"color:MediumOrchid\">LlamaIndex</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,  Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"llama3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[1;32m    197\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[1;32m    198\u001b[0m     )\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_backends/sync.py:124\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    123\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {socket\u001b[38;5;241m.\u001b[39mtimeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_query_engine()\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m query_engine\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarise What is Llama3 and make it SEO optimized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py:53\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     52\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 53\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query(str_or_query_bundle)\n\u001b[1;32m     54\u001b[0m dispatch_event(QueryEndEvent())\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py:190\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    187\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    188\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    189\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 190\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_synthesizer\u001b[38;5;241m.\u001b[39msynthesize(\n\u001b[1;32m    191\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery_bundle,\n\u001b[1;32m    192\u001b[0m         nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    194\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/response_synthesizers/base.py:242\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    239\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE,\n\u001b[1;32m    240\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    241\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 242\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m    243\u001b[0m         query_str\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery_str,\n\u001b[1;32m    244\u001b[0m         text_chunks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    245\u001b[0m             n\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mLLM) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    246\u001b[0m         ],\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    251\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:43\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_compact_text_chunks(query_str, text_chunks)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m     44\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     45\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[1;32m     46\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m     48\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py:183\u001b[0m, in \u001b[0;36mRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text_chunk \u001b[38;5;129;01min\u001b[39;00m text_chunks:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;66;03m# if this is the first chunk, and text chunk already\u001b[39;00m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;66;03m# is an answer, then return it\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_give_response_single(\n\u001b[1;32m    184\u001b[0m             query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# refine response if possible\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_response_single(\n\u001b[1;32m    189\u001b[0m             prev_response, query_str, text_chunk, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs\n\u001b[1;32m    190\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py:238\u001b[0m, in \u001b[0;36mRefine._give_response_single\u001b[0;34m(self, query_str, text_chunk, **response_kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m         structured_response \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    237\u001b[0m             StructuredRefineResponse,\n\u001b[0;32m--> 238\u001b[0m             program(\n\u001b[1;32m    239\u001b[0m                 context_str\u001b[38;5;241m=\u001b[39mcur_text_chunk,\n\u001b[1;32m    240\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m    241\u001b[0m             ),\n\u001b[1;32m    242\u001b[0m         )\n\u001b[1;32m    243\u001b[0m         query_satisfied \u001b[38;5;241m=\u001b[39m structured_response\u001b[38;5;241m.\u001b[39mquery_satisfied\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m query_satisfied:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/response_synthesizers/refine.py:84\u001b[0m, in \u001b[0;36mDefaultRefineProgram.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     82\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prompt,\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructuredRefineResponse(answer\u001b[38;5;241m=\u001b[39manswer, query_satisfied\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:274\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[1;32m    271\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/llms/llm.py:430\u001b[0m, in \u001b[0;36mLLM.predict\u001b[0;34m(self, prompt, **prompt_args)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[1;32m    429\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_args)\n\u001b[0;32m--> 430\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat(messages)\n\u001b[1;32m    431\u001b[0m     output \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:144\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    129\u001b[0m     LLMChatStartEvent(\n\u001b[1;32m    130\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m_self\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    137\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    138\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     },\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/llama_index/llms/ollama/base.py:101\u001b[0m, in \u001b[0;36mOllama.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m }\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mClient(timeout\u001b[38;5;241m=\u001b[39mTimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_timeout)) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[0;32m--> 101\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    102\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    103\u001b[0m         json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    106\u001b[0m     raw \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1145\u001b[0m, in \u001b[0;36mClient.post\u001b[0;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1126\u001b[0m     url: URLTypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     extensions: RequestExtensions \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1139\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m    Send a `POST` request.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \n\u001b[1;32m   1143\u001b[0m \u001b[38;5;124;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1147\u001b[0m         url,\n\u001b[1;32m   1148\u001b[0m         content\u001b[38;5;241m=\u001b[39mcontent,\n\u001b[1;32m   1149\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m   1150\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m   1151\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[1;32m   1152\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m   1153\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1154\u001b[0m         cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[1;32m   1155\u001b[0m         auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m   1156\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m   1157\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1158\u001b[0m         extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m   1159\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    826\u001b[0m )\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, auth\u001b[38;5;241m=\u001b[39mauth, follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[1;32m    915\u001b[0m     request,\n\u001b[1;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[1;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    919\u001b[0m )\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[1;32m    943\u001b[0m         request,\n\u001b[1;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[1;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mReadTimeout\u001b[0m: timed out"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Summarise What is Llama3 and make it SEO\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
